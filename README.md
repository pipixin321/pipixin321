<h2><em> Hi there ðŸ‘‹, I'm Huaxin Zhang</em></h2>

<p> 
<a href="https://pipixin321.github.io/"><img src="https://img.shields.io/badge/Huaxin_Zhang-Homepage-orange" height="25px" alt="Huaxin Zhang"></a>
<a href="https://github.com/pipixin321"><img src="https://img.shields.io/badge/github-%23121011.svg?style=flat-square&logo=github&logoColor=white" height="25px" alt="github"></a>
<a href="https://scholar.google.com.hk/citations?user=oyfu0pgAAAAJ&hl=zh-CN"><img src="https://img.shields.io/badge/Google%20Scholar-4285F4?style=flat-square&logo=google-scholar&logoColor=white" height="25px" alt="Google Scholar"></a>
</p>


I am a Master of HUST (Huazhong University of Science and Technology), supervised by Prof. Changxin Gao and Prof. Nong Sang.

ðŸ”­ **Reseach-wise, I mainly focus on**:
- Multi-modal Large Language Models
- Video Understanding, more specifically, Weakly-supervised Temporal Action Localization (WSTAL) & Weakly-suervised Video Anomaly Detection (WSVAD).


ðŸ˜„ **I am open to**:

- A internship/job/PhD offer with computer vision/multimodal LLM research and engineering.

ðŸ“« **Contact me by**:

- Email: zhanghuaxin@hust.edu.cn

ðŸ’¬ **News**:
  - 2024-07-01: We release our code and model of "Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM".[[project page](https://holmesvad.github.io/)]
  - 2024-06-10: We release our code and model of "Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities".[[project page](https://arcana-project-page.github.io/)]
  - 2024-01-29: I start my internship in Baidu VIS, to do some research on Multi-modal Large Language Model (MLLM).
  - 2023-12-09: One paper about point supervised temporal action localization is accepted on AAAI 2024.




----

[![Huaxin's github stats](https://github-readme-stats.vercel.app/api?username=pipixin321&theme=material-palenight&count_private=true&hide=contribs)](https://github.com/anuraghazra/github-readme-stats)

